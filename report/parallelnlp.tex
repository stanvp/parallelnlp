\documentclass{report}
\usepackage{fullpage}
\usepackage{cite}
\renewcommand{\baselinestretch}{2}
\author{Stanislav Peshterliev}
\title{Parallel NLP Algorithms in Scala}
\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}

14.01.2011 - deadline 

20-30 pages

Parallel Programming method

From formalism to implementation

Tunning

Results

How to extend and reusability

\cite{berger_a1-etal:1996a}
\cite{conf/nips/MannMMSW09}

\section{Your Section title Here}
\subsection{Your Subsection title here}
\subsubsection{Your subsubsection title here}
\paragraph{Your paragraph title here}

\chapter{Algorithms}

\chapter{Text classification}

\chapter{Implementation}

\section{Training corpus compression}

The training corpus that are used comes in plain text or xml formats. Parsing large number of files is time consuming and does not give information regarding the performance of the algorithm and only may obfuscate performance results, and because the algorithms are run many time against the same data minimizing the parsing time and I/O time is important. That is why training data is parsed and compressed in advanced.

First, all training data is loaded and feature selection is performed, after the most useful features are decided each features is given a unique id, and the triple id, features, and information gain is written to a files, see figure \ref{featureextract}.

\begin{figure}[h!]
 \label{featureextract}
  \centering 
2507 $|$ bad $|$ -0.2579069473576291 \\
3081 $|$ worst $|$ -0.2638446978715786  \\
1425 $|$ stupid $|$ -0.2752135575766913 \\
8286 $|$ boring $|$ -0.27746205051618666 \\
1110 $|$ waste $|$ -0.2813770454287048 \\
.......
  \caption{Features file extract}
\end{figure}

\chapter{Results}

\chapter{How to extend and reusability}

\bibliography{references}{}
\bibliographystyle{plain}

\end{document}